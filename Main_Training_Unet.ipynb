{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Main_Training_Unet.ipynb","provenance":[{"file_id":"1X3jS_tzln8bvRpM030RPUayY8FzfMJ4f","timestamp":1607979738585}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iUszbrq6Lbj_"},"source":["NAME_1: Marco </br>\r\n","LAST-NAME_1: Introvigne </br>\r\n","STUDENT-ID_1: 10750466 </br>\r\n","NAME_2: Antonio </br>\r\n","LAST-NAME_2: Urbano </br>\r\n","STUDENT-ID_2: 10527285 </br>\r\n","NAME_3: Enrico </br>\r\n","LAST-NAME_3: Voltan </br>\r\n","STUDENT-ID_3: 10525467 </br>\r\n","LEADERBOARD NICKNAME: 3LN (Three Little Neurons) </br>\r\n"]},{"cell_type":"markdown","metadata":{"id":"Xl6JoQO_Ljec"},"source":["This notebook is the one used to train all the models of all the companies except of Roseau. We tuned differrent parameters for each model, this is just an example of the architecture applied to Pead company dataset"]},{"cell_type":"markdown","metadata":{"id":"PRtubD_RQy_e"},"source":["# Training UNET\n"]},{"cell_type":"code","metadata":{"id":"mK5uCC-xv9cj"},"source":["from IPython.core.interactiveshell import InteractiveShell\r\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnDvS58Lv-HJ"},"source":["import pandas as pd\r\n","import os\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","import shutil, random\r\n","import time\r\n","import matplotlib.pyplot as plt\r\n","from tensorflow import keras\r\n","from PIL import Image\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","from matplotlib import cm\r\n","from tensorflow.keras.applications.vgg16 import preprocess_input \r\n","\r\n","SEED = 1234\r\n","tf.random.set_seed(SEED)\r\n","cwd = os.getcwd() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hz4WN6xwFOp","executionInfo":{"status":"ok","timestamp":1608588781826,"user_tz":-60,"elapsed":2610,"user":{"displayName":"Enrico Voltan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxPC8hIjq-aZT3E_J2qVVrmoBqgb7ioMLDcn-_QUY=s64","userId":"02961495656321921574"}},"outputId":"f2541912-5d58-4ac5-f8b1-2c8b6786d486"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ok8DlakHjeKQ"},"source":["## Parameters"]},{"cell_type":"markdown","metadata":{"id":"SDbdq1gWVlLI"},"source":["Here we can set some parameters such as the image size, whether or not to use the data augmentation and the size of the batch size"]},{"cell_type":"code","metadata":{"id":"0NV3oNO5xVhj"},"source":["apply_data_augmentation = True\r\n","img_h = 768\r\n","img_w = 768\r\n","batch_size = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ufHzrQ-Jm4Kd"},"source":["## Dataset\r\n"]},{"cell_type":"markdown","metadata":{"id":"8IiCJoLYX1RV"},"source":["Three different methods:\r\n","- read_names: Find all the images from the images_path directory\r\n","- read_rgb_mask: Returns the numpy array containing target values\r\n","- cropImg: Divide images by the number of columns and rows entered\r\n"]},{"cell_type":"code","metadata":{"id":"EUBA5lFAfpBy"},"source":["import math\r\n","\r\n","def read_names(images_path):\r\n","    \"\"\" Find all the images from the images_path directory\"\"\"\r\n","\r\n","    image_files = []\r\n","\r\n","    for dir_entry in os.listdir(images_path):\r\n","        if os.path.isfile(os.path.join(images_path, dir_entry)):\r\n","            image_files.append(dir_entry)\r\n","\r\n","    return image_files\r\n","\r\n","\r\n","def read_rgb_mask(img_path, out_shape, crop_part, nrow, ncol):\r\n","    \"\"\" img_path: path to the mask file\r\n","    Returns the numpy array containing target values \"\"\"\r\n","\r\n","    mask_img = Image.open(img_path)\r\n","    mask_img = cropImg(mask_img, nrow, ncol)[crop_part]\r\n","    mask_img = mask_img.resize(out_shape, resample=Image.NEAREST)\r\n","    mask_arr = np.array(mask_img)\r\n","\r\n","    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\r\n","\r\n","    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\r\n","    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\r\n","    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\r\n","\r\n","    return new_mask_arr\r\n","    \r\n","def cropImg(im, rows, cols):\r\n","    \"\"\" Divide images by the number of columns and rows entered \"\"\"\r\n","    # im = Image.open(infile)\r\n","    width = math.ceil(im.size[0]/cols)\r\n","    height = math.ceil(im.size[1]/rows)\r\n","    parts = []\r\n","    for i in range(rows):\r\n","        for j in range(cols):\r\n","            # print (i,j)\r\n","            box = (j*width, i*height, (j+1)*width, (i+1)*height)\r\n","            parts.append(im.crop(box))\r\n","    return parts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImyNJsgjYpyr"},"source":["This class is useful for creating the training dataset and training validation. The parameters to be set are the desired directory, if we want to use img_generator and mask_generator, the number of rows and columns in which to divide the image and the output shape."]},{"cell_type":"code","metadata":{"id":"n8vbbSOIYehA"},"source":["class ParametricCustomDataset(tf.keras.utils.Sequence):\r\n","\r\n","  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None,\r\n","               preprocessing_function=None, nrow=1, ncol=1, out_shape=(512,512)):\r\n","    \r\n","    subset_filenames = read_names(os.path.join(dataset_dir, \"Images/img\"))\r\n","        \r\n","    self.which_subset = which_subset\r\n","    self.dataset_dir = dataset_dir    \r\n","    self.subset_filenames = subset_filenames\r\n","    self.img_generator = img_generator\r\n","    self.mask_generator = mask_generator\r\n","    self.preprocessing_function = preprocessing_function\r\n","    self.nrow=nrow\r\n","    self.ncol=ncol\r\n","    self.out_shape = out_shape\r\n","\r\n","  def __len__(self):\r\n","    return len(self.subset_filenames * self.nrow * self.ncol)\r\n","\r\n","  def __getitem__(self, index):\r\n","    curr_filename = self.subset_filenames[index%len(self.subset_filenames)]\r\n","    img = Image.open(os.path.join(self.dataset_dir,  'Images/img', curr_filename))\r\n","    x = index // len(self.subset_filenames)\r\n","    img = cropImg(img, self.nrow, self.ncol)[x]\r\n","    img = img.resize(self.out_shape) \r\n","    img_arr = np.array(img)\r\n","\r\n","    mask = read_rgb_mask(os.path.join(self.dataset_dir,  'Masks/img', curr_filename[:-4] + '.png'),\r\n","    self.out_shape, x, self.nrow, self.ncol)   \r\n","\r\n","    mask_arr = np.array(mask)\r\n","    mask_arr = np.expand_dims(mask_arr, -1) \r\n","\r\n","    if \"training\" in self.which_subset and self.img_generator is not None and self.mask_generator is not None:\r\n","\r\n","        img_t = self.img_generator.get_random_transform(img_arr.shape)\r\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\r\n","\r\n","        out_mask = np.zeros_like(mask_arr)\r\n","        for c in np.unique(mask_arr):\r\n","          if c > 0:\r\n","            curr_class_arr = np.float32(mask_arr == c)\r\n","            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, img_t)\r\n","            \r\n","            curr_class_arr = np.uint8(curr_class_arr)\r\n","            \r\n","            curr_class_arr = curr_class_arr * c \r\n","            out_mask += curr_class_arr\r\n","    else:\r\n","      out_mask = mask_arr\r\n","    \r\n","    if self.preprocessing_function is not None:\r\n","        img_arr = self.preprocessing_function(img_arr)\r\n","\r\n","    return img_arr.astype(float)/255, out_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LGgLm8-AyPa"},"source":["# ImageDataGenerator\n","if apply_data_augmentation:\n","    img_data_gen = ImageDataGenerator(rotation_range=360,\n","                                      width_shift_range=0.4,\n","                                      height_shift_range=0.4,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect')\n","    mask_data_gen = ImageDataGenerator(rotation_range=360,\n","                                       width_shift_range=0.4,\n","                                       height_shift_range=0.4,\n","                                       horizontal_flip=True,\n","                                       vertical_flip=True,\n","                                       fill_mode='reflect')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfZTQNZwHemU"},"source":["path_tr = '/content/drive/MyDrive/Kaggle_2/Development_Dataset_Split/training/Pead/Haricot'\r\n","path_val = '/content/drive/MyDrive/Kaggle_2/Development_Dataset_Split/validation/Pead/Haricot'\r\n","\r\n","dataset_train = ParametricCustomDataset(path_tr, 'training',\r\n","                                        img_generator=img_data_gen,\r\n","                                        mask_generator=mask_data_gen,\r\n","                                        nrow=2, ncol=2,\r\n","                                        out_shape=(img_w,img_h),\r\n","                                        preprocessing_function=None)\r\n","dataset_valid = ParametricCustomDataset(path_val, 'validation',\r\n","                                        nrow=2, ncol=2,\r\n","                                        out_shape=(img_w,img_h),\r\n","                                        preprocessing_function=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7VlJoYNHh66"},"source":["train_dataset = tf.data.Dataset.from_generator(lambda: dataset_train,\r\n","                                               output_types=(tf.float32, tf.float32),\r\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n","train_dataset = train_dataset.batch(batch_size)\r\n","train_dataset = train_dataset.repeat()\r\n","\r\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\r\n","                                               output_types=(tf.float32, tf.float32),\r\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w, 1]))\r\n","valid_dataset = valid_dataset.batch(batch_size)\r\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBLscU0Q4EQT"},"source":["## Test Data Generator"]},{"cell_type":"code","metadata":{"id":"N6XSCM18b1eE"},"source":["iterator = iter(train_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYp_BuZYOsuW"},"source":["%matplotlib inline\r\n","colors = []\r\n","colors.append([0, 0, 0]) # backgroun\r\n","colors.append([255, 255, 255] ) # crop\r\n","colors.append([216, 67, 82])  # weed\r\n","\r\n","fig, ax = plt.subplots(1, 2, figsize=(10,10))\r\n","\r\n","augmented_img, target = next(iterator)\r\n","augmented_img = np.array(augmented_img[0])  # First element\r\n","\r\n","target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\r\n","\r\n","print(np.unique(target))\r\n","\r\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\r\n","\r\n","target_img[np.where(target == 0)] = [0, 0, 0]\r\n","target_img[np.where(target == 1)] = colors[1]\r\n","target_img[np.where(target == 2)] = colors[2]\r\n","\r\n","ax[0].imshow(np.uint8(augmented_img * 255))\r\n","ax[1].imshow(np.uint8(target_img))\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JARQYqiE5Tcz"},"source":["## UNET"]},{"cell_type":"code","metadata":{"id":"YFNRet0GbJYJ"},"source":["from drive.MyDrive.Kaggle_2.Unet import build_Unet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5X-kdxTWJ8a9"},"source":["The inputs to the build_Unet method are the input shape, the initial number of filters, the image size, the model depth, whether or not to use the batch_normalization and the dropout after each single layer.\r\n","We obtained the best results with base_filter = 64, depth of 4 or 5, dropout of 0.4 and the use of batch_normalization and Conv2Dtranspose in the decoder block"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpr5C1tHhmoa","executionInfo":{"status":"ok","timestamp":1608541164390,"user_tz":-60,"elapsed":1460,"user":{"displayName":"Enrico Voltan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxPC8hIjq-aZT3E_J2qVVrmoBqgb7ioMLDcn-_QUY=s64","userId":"02961495656321921574"}},"outputId":"ae9dd6f0-fdd3-4234-85ba-e1f2470538f0"},"source":["model = build_Unet(input_shape=(img_h, img_w, 3), output_channels=3, depth=5, base_filters = 64)\r\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 768, 768, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 768, 768, 64) 1792        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 768, 768, 64) 256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 768, 768, 64) 0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 768, 768, 64) 36928       dropout[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 768, 768, 64) 256         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 384, 384, 64) 0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 384, 384, 128 73856       max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 384, 384, 128 512         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 384, 384, 128 0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 384, 384, 128 147584      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 384, 384, 128 512         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 192, 192, 128 0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 192, 192, 256 295168      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 192, 192, 256 1024        conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 192, 192, 256 0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 192, 192, 256 590080      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 192, 192, 256 1024        conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 96, 96, 256)  0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 96, 96, 512)  1180160     max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 96, 96, 512)  2048        conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 96, 96, 512)  0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 96, 96, 512)  2359808     dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 96, 96, 512)  2048        conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 48, 48, 512)  0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 48, 48, 1024) 4719616     max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 48, 48, 1024) 4096        conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 48, 48, 1024) 0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 48, 48, 1024) 9438208     dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 48, 48, 1024) 4096        conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_4 (MaxPooling2D)  (None, 24, 24, 1024) 0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 24, 24, 2048) 18876416    max_pooling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 24, 24, 2048) 8192        conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 24, 24, 2048) 0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 24, 24, 2048) 37750784    dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 24, 24, 2048) 8192        conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose (Conv2DTranspo (None, 48, 48, 1024) 8389632     batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 48, 48, 2048) 0           conv2d_transpose[0][0]           \n","                                                                 batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 48, 48, 1024) 18875392    concatenate[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 48, 48, 1024) 4096        conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 48, 48, 1024) 0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 48, 48, 1024) 9438208     dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 48, 48, 1024) 4096        conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 96, 96, 512)  2097664     batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 96, 96, 1024) 0           conv2d_transpose_1[0][0]         \n","                                                                 batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 96, 96, 512)  4719104     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 96, 96, 512)  2048        conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 96, 96, 512)  0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 96, 96, 512)  2359808     dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 96, 96, 512)  2048        conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_2 (Conv2DTrans (None, 192, 192, 256 524544      batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 192, 192, 512 0           conv2d_transpose_2[0][0]         \n","                                                                 batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 192, 192, 256 1179904     concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 192, 192, 256 1024        conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 192, 192, 256 0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 192, 192, 256 590080      dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 192, 192, 256 1024        conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_3 (Conv2DTrans (None, 384, 384, 128 131200      batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 384, 384, 256 0           conv2d_transpose_3[0][0]         \n","                                                                 batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 384, 384, 128 295040      concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 384, 384, 128 512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 384, 384, 128 0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 384, 384, 128 147584      dropout_9[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 384, 384, 128 512         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_4 (Conv2DTrans (None, 768, 768, 64) 32832       batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 768, 768, 128 0           conv2d_transpose_4[0][0]         \n","                                                                 batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 768, 768, 64) 73792       concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 768, 768, 64) 256         conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_10 (Dropout)            (None, 768, 768, 64) 0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 768, 768, 64) 36928       dropout_10[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 768, 768, 3)  195         conv2d_21[0][0]                  \n","==================================================================================================\n","Total params: 124,410,179\n","Trainable params: 124,386,243\n","Non-trainable params: 23,936\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uzbAqJEk5iOd"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"ibha1zoGeIMw"},"source":["loss = tf.keras.losses.SparseCategoricalCrossentropy() \r\n","lr = 0.001\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\r\n","\r\n","def meanIoU(y_true, y_pred):\r\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\r\n","\r\n","    per_class_iou = []\r\n","\r\n","    for i in range(1,3):\r\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\r\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\r\n","      intersection = tf.reduce_sum(class_true * class_pred)\r\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\r\n","    \r\n","      iou = (intersection + 1e-7) / (union + 1e-7)\r\n","      per_class_iou.append(iou)\r\n","\r\n","    return tf.reduce_mean(per_class_iou)\r\n","\r\n","metrics = ['accuracy', meanIoU]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OeuyXpHeMGY"},"source":["model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMbYNa5Thznz"},"source":["trained_model = model.fit(x=train_dataset,\r\n","                          epochs=100, \r\n","                          steps_per_epoch=76 * 4,\r\n","                          validation_data=valid_dataset,\r\n","                          validation_steps=14 * 4, \r\n","                          callbacks=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ub-PqJ5k2sr6"},"source":["model.save_weights(\"/content/drive/MyDrive/Kaggle_2/HAR_PEAD.h5\")"],"execution_count":null,"outputs":[]}]}