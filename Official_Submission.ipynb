{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Official_Submission.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QjtXJKXbROIq"},"source":["NAME_1: Marco </br>\r\n","LAST-NAME_1: Introvigne </br>\r\n","STUDENT-ID_1: 10750466 </br>\r\n","NAME_2: Antonio </br>\r\n","LAST-NAME_2: Urbano </br>\r\n","STUDENT-ID_2: 10527285 </br>\r\n","NAME_3: Enrico </br>\r\n","LAST-NAME_3: Voltan </br>\r\n","STUDENT-ID_3: 10525467 </br>\r\n","LEADERBOARD NICKNAME: 3LN (Three Little Neurons) </br>"]},{"cell_type":"markdown","metadata":{"id":"DU0tzinXRP9i"},"source":["For each combination of type and company, we define the ‘template’ of the model used during training, we load the saved weights and finally we perform the prediction."]},{"cell_type":"code","metadata":{"id":"suPCUubZVb79"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cpqm7bT6V2I2"},"source":["import pandas as pd\n","import os\n","import tensorflow as tf\n","import numpy as np\n","import shutil, random\n","import time\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from matplotlib import cm\n","from tensorflow.keras.applications.vgg16 import preprocess_input \n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","cwd = os.getcwd() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJIl8kc3V2Pa","executionInfo":{"status":"ok","timestamp":1609069760332,"user_tz":-60,"elapsed":1004,"user":{"displayName":"Enrico Voltan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjxPC8hIjq-aZT3E_J2qVVrmoBqgb7ioMLDcn-_QUY=s64","userId":"02961495656321921574"}},"outputId":"9c9d1a8e-ee9b-45a4-adbd-fb590f0f2357"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4hO2dx0jRzHW"},"source":["## VGG Model"]},{"cell_type":"code","metadata":{"id":"T7U4M2yQUY1Y"},"source":["vgg = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(832, 832, 3))\n","def vggNet(depth, start_f, num_classes):\n","\n","    model = tf.keras.Sequential()\n","    \n","    # Encoder\n","    model.add(vgg)\n","    \n","        \n","    # Decoder\n","    for i in range(depth):\n","        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n","        model.add(tf.keras.layers.Conv2D(filters=start_f,\n","                                         kernel_size=(3, 3),\n","                                         strides=(1, 1),\n","                                         padding='same'))\n","        model.add(tf.keras.layers.ReLU())\n","\n","        start_f = start_f // 2\n","\n","    # Prediction Layer\n","    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n","                                     kernel_size=(1, 1),\n","                                     strides=(1, 1),\n","                                     padding='same',\n","                                     activation='softmax'))\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gID1eHOnAH8m"},"source":["loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","lr = 0\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","def meanIoU(y_true, y_pred):\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3):\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","metrics = ['accuracy', meanIoU]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMFoUVQQR5m1"},"source":["## UNET"]},{"cell_type":"code","metadata":{"id":"71r1Sukl0tPn"},"source":["from drive.MyDrive.Kaggle_2.Unet import build_Unet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-fIIpyxR7t-"},"source":["## Build Model"]},{"cell_type":"code","metadata":{"id":"TrYP0BAKAMRI"},"source":["haricot_weedelec_model = build_Unet(input_shape=(768, 768, 3), output_channels=3, depth=5,  base_filters  = 64)\n","haricot_weedelec_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","haricot_weedelec_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/HAR_WEED.h5\")\n","\n","haricot_pead_model = build_Unet(input_shape=(768, 768, 3), output_channels=3, depth=5,  base_filters  = 64)\n","haricot_pead_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","haricot_pead_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/HAR_PEAD.h5\")\n","\n","mais_weedelec_model = build_Unet(input_shape=(512, 512, 3), output_channels=3, depth=6,  base_filters  = 64)\n","mais_weedelec_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","mais_weedelec_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/MAIS_WEED.h5\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtRSKxot2umE"},"source":["mais_pead_model = build_Unet(input_shape=(512, 512, 3), output_channels=3, depth=5,  base_filters  = 64)\n","mais_pead_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","mais_pead_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/MAIS_PEAD.h5\")\n","\n","mais_bipbip_model = build_Unet(input_shape=(512, 512, 3), output_channels=3, depth=4,  base_filters  = 64)\n","mais_bipbip_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","mais_bipbip_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/MAIS-BIPBIP.h5\")\n","\n","haricot_bipbip_model = build_Unet(input_shape=(512, 512, 3), output_channels=3, depth=4,  base_filters  = 64)\n","haricot_bipbip_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","haricot_bipbip_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/HAR-BIPBIP.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wD108LmpoCC"},"source":["mais_roseau_model = vggNet(depth=5, \n","                     start_f=832, \n","                     num_classes=3)\n","mais_roseau_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","mais_roseau_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/MAIS-ROS.h5\")\n","\n","haricot_roseau_model = vggNet(depth=5, \n","                     start_f=832, \n","                     num_classes=3)\n","haricot_roseau_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","haricot_roseau_model.load_weights(\"/content/drive/MyDrive/Kaggle_2/HAR_ROS.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rOHjTMBcSC0S"},"source":["## Dictionary"]},{"cell_type":"markdown","metadata":{"id":"8CFCj9EjSHh6"},"source":["We have created a dictionary in which we join the different results obtained by the single models. "]},{"cell_type":"code","metadata":{"id":"nJA7i7y61h6g"},"source":["input_dict = {}\n","input_dict[\"Weedelec\"] = {}\n","input_dict[\"Weedelec\"][\"Haricot\"] = 768\n","input_dict[\"Weedelec\"][\"Mais\"] = 512\n","\n","input_dict[\"Bipbip\"] = {}\n","input_dict[\"Bipbip\"][\"Haricot\"] = 512\n","input_dict[\"Bipbip\"][\"Mais\"] = 512\n","\n","input_dict[\"Roseau\"] = {}\n","input_dict[\"Roseau\"][\"Haricot\"] = 832\n","input_dict[\"Roseau\"][\"Mais\"] = 832\n","\n","input_dict[\"Pead\"] = {}\n","input_dict[\"Pead\"][\"Haricot\"] = 768\n","input_dict[\"Pead\"][\"Mais\"] = 512\n","\n","model_dict = {}\n","model_dict[\"Weedelec\"] = {}\n","model_dict[\"Weedelec\"][\"Haricot\"] = haricot_weedelec_model\n","model_dict[\"Weedelec\"][\"Mais\"] = mais_weedelec_model\n","\n","model_dict[\"Bipbip\"] = {}\n","model_dict[\"Bipbip\"][\"Haricot\"] = haricot_bipbip_model\n","model_dict[\"Bipbip\"][\"Mais\"] = mais_bipbip_model\n","\n","model_dict[\"Roseau\"] = {}\n","model_dict[\"Roseau\"][\"Haricot\"] = haricot_roseau_model\n","model_dict[\"Roseau\"][\"Mais\"] = mais_roseau_model\n","\n","model_dict[\"Pead\"] = {}\n","model_dict[\"Pead\"][\"Haricot\"] = haricot_pead_model\n","model_dict[\"Pead\"][\"Mais\"] = mais_pead_model\n","\n","split_dict = {}\n","split_dict[\"Weedelec\"] = {}\n","split_dict[\"Weedelec\"][\"Haricot\"] ={}\n","split_dict[\"Weedelec\"][\"Haricot\"][\"cols\"] = 2\n","split_dict[\"Weedelec\"][\"Haricot\"][\"rows\"] = 2\n","split_dict[\"Weedelec\"][\"Mais\"] ={}\n","split_dict[\"Weedelec\"][\"Mais\"][\"cols\"] = 2\n","split_dict[\"Weedelec\"][\"Mais\"][\"rows\"] = 2\n","\n","split_dict[\"Bipbip\"] = {}\n","split_dict[\"Bipbip\"][\"Haricot\"] ={}\n","split_dict[\"Bipbip\"][\"Haricot\"][\"cols\"] = 2\n","split_dict[\"Bipbip\"][\"Haricot\"][\"rows\"] = 2\n","split_dict[\"Bipbip\"][\"Mais\"] ={}\n","split_dict[\"Bipbip\"][\"Mais\"][\"cols\"] = 2\n","split_dict[\"Bipbip\"][\"Mais\"][\"rows\"] = 2\n","\n","split_dict[\"Roseau\"] = {}\n","split_dict[\"Roseau\"][\"Haricot\"] ={}\n","split_dict[\"Roseau\"][\"Haricot\"][\"cols\"] = 2\n","split_dict[\"Roseau\"][\"Haricot\"][\"rows\"] = 1\n","split_dict[\"Roseau\"][\"Mais\"] ={}\n","split_dict[\"Roseau\"][\"Mais\"][\"cols\"] = 2\n","split_dict[\"Roseau\"][\"Mais\"][\"rows\"] = 1\n","\n","split_dict[\"Pead\"] = {}\n","split_dict[\"Pead\"][\"Haricot\"] ={}\n","split_dict[\"Pead\"][\"Haricot\"][\"cols\"] = 2\n","split_dict[\"Pead\"][\"Haricot\"][\"rows\"] = 2\n","split_dict[\"Pead\"][\"Mais\"] ={}\n","split_dict[\"Pead\"][\"Mais\"][\"cols\"] = 2\n","split_dict[\"Pead\"][\"Mais\"][\"rows\"] = 2\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9o82LVarvk3"},"source":["def read_names(images_path):\n","    \"\"\" Find all the images from the images_path directory and\n","        the segmentation images from the segs_path directory\n","        while checking integrity of data \"\"\"\n","\n","    image_files = []\n","    print(images_path)\n","    for dir_entry in os.listdir(images_path):\n","        if os.path.isfile(os.path.join(images_path, dir_entry)):\n","            image_files.append(dir_entry)\n","\n","    return image_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6nzrNNh7q0-V"},"source":["import math\n","def cropImg(im, rows, cols): \n","  width = math.ceil(im.size[0]/cols)\n","  height = math.ceil(im.size[1]/rows)\n","  parts = []\n","  for i in range(rows):\n","    for j in range(cols):\n","      right_w = (j+1)*width if j < cols - 1 else im.size[0]\n","      right_h = (i+1)*height if i < rows - 1 else im.size[1]\n","      box = (j*width, i*height, right_w, right_h)\n","      parts.append(im.crop(box))\n","  return parts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zi7gIo3b-IXE"},"source":["class CustomTestDataset(tf.keras.utils.Sequence):\n","\n","  def __init__(self, dataset_dir, out_shape=(800,800), cols = 2, rows = 2, preprocess = None):\n","        \n","    subset_filenames = read_names(dataset_dir)   \n","    print(len(subset_filenames))\n","    self.dataset_dir = dataset_dir    \n","    self.subset_filenames = subset_filenames\n","    self.out_shape = out_shape\n","    self.cols = cols\n","    self.rows = rows\n","    self.pre = preprocess\n","  \n","\n","  def __len__(self):\n","    return len(self.subset_filenames)* self.cols * self.rows\n","\n","  def __getitem__(self, index):\n","    curr_filename = self.subset_filenames[index // (self.cols * self.rows)]\n","    img = Image.open(os.path.join(self.dataset_dir, curr_filename))\n","    x = index % (self.cols * self.rows)\n","    img = cropImg(img,self.rows,self.cols)[x]\n","    img = img.resize(self.out_shape) \n","    img_arr = np.array(img)\n","    if self.pre is not None:\n","      img_arr = self.pre(img_arr)\n","\n","    return img_arr.astype(float)/255, np.array([curr_filename])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i2wZMK_wnVj6"},"source":["def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jW-vKCX7S1AK"},"source":["colors = []\n","colors.append([0, 0, 0]) # background\n","colors.append([255, 255, 255] ) # crop\n","colors.append([216, 67, 82])  # weed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SW_jnODnhdMW"},"source":["n_classes = 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLVjFRBbSMWq"},"source":["Two methods used to join the cropped predictions in order to obtain the original one"]},{"cell_type":"code","metadata":{"id":"Q3Rb2b1fJYjp"},"source":["def join_hor(im1, im2):\n","   imgs = [ im1, im2 ]\n","   min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n","   imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n","   return Image.fromarray(imgs_comb)\n","\n","def join_vert(im1, im2):\n","   imgs = [ im1, im2 ]\n","   min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n","   imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n","   return Image.fromarray(imgs_comb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6wqU2BZ6SaDB"},"source":["## Create .json"]},{"cell_type":"code","metadata":{"id":"WcAcDFMUYOPH"},"source":["import json\r\n","from tensorflow.keras.applications.vgg19 import preprocess_input \r\n","\r\n","aziende = [\"Bipbip\", \"Pead\", \"Roseau\", \"Weedelec\"]\r\n","tipi = [\"Mais\",\"Haricot\"]\r\n","submission_dict = {}\r\n","for azienda in aziende:\r\n","  for tipo in tipi:\r\n","    input_height = input_dict[azienda][tipo]\r\n","    output_height = input_dict[azienda][tipo]\r\n","    model = model_dict[azienda][tipo]\r\n","    cols = split_dict[azienda][tipo][\"cols\"]\r\n","    rows = split_dict[azienda][tipo][\"rows\"]\r\n","    preprocess = preprocess_input if azienda == \"Roseau\" else None # and tipo == \"Haricot\"\r\n","\r\n","\r\n","    test_path = f\"/content/drive/MyDrive/Kaggle_2/Development_Dataset/Test_Dev/{azienda}/{tipo}/Images\"\r\n","    ds = CustomTestDataset(test_path,(input_height,input_height), cols, rows, preprocess)\r\n","    test_dataset = tf.data.Dataset.from_generator(lambda: ds,\r\n","                                               output_types=(tf.float32, tf.string),\r\n","                                               output_shapes=([input_height, input_height, 3],[1,]))\r\n","\r\n","    test_dataset = test_dataset.repeat()\r\n","\r\n","    iterator = iter(test_dataset)\r\n","    for t in range(15):      \r\n","      parts = []\r\n","      for i in range(rows):\r\n","        for j in range(cols):\r\n","          image, img_name = next(iterator)\r\n","          #out_sigmoid = model.predict(x=tf.expand_dims(image, 0)) -> prediction without computing any rotation\r\n","          \r\n","          # We perform 4 rotation of 90° to all the cropped part\r\n","          out_sigmoid_0 = model.predict(x=tf.expand_dims(image, 0))\r\n","          out_sigmoid_1 = model.predict(x=tf.expand_dims(tf.image.rot90(image,1), 0))\r\n","          out_sigmoid_2 = model.predict(x=tf.expand_dims(tf.image.rot90(image,2), 0))\r\n","          out_sigmoid_3 = model.predict(x=tf.expand_dims(tf.image.rot90(image,3), 0))\r\n","\r\n","          # Sum of the prediction\r\n","          out_sigmoid = tf.image.rot90(out_sigmoid_3,1) + tf.image.rot90(out_sigmoid_2,2) + tf.image.rot90(out_sigmoid_1,3) + out_sigmoid_0\r\n","          predicted_class = tf.argmax(out_sigmoid, -1)\r\n","          predicted_class = predicted_class[0, ...]\r\n","          prediction_img = np.zeros([predicted_class.shape[0], predicted_class.shape[1], 3])\r\n","          prediction_img[np.where(predicted_class == 0)] = [0, 0, 0]\r\n","          for i in range(1, 3):\r\n","            prediction_img[np.where(predicted_class == i)] = np.array(colors[i])\r\n","          png = Image.fromarray(np.uint8(prediction_img),\"RGB\")\r\n","          parts.append(png)\r\n","      print(f\"collected {len(parts)}\")\r\n","      \r\n","      # Here we join all the cropped predictions\r\n","      finalPrediction = None\r\n","      for r in range(rows):\r\n","        c = 1\r\n","        r_img = parts[r * cols]\r\n","        while c < cols:\r\n","          r_img = join_hor(r_img, parts[r * cols + c])\r\n","          c +=1\r\n","        if r == 0:\r\n","          finalPrediction = r_img\r\n","        else:\r\n","          finalPrediction = join_vert(finalPrediction, r_img)\r\n","        \r\n","      print(finalPrediction.size)\r\n","\r\n","      \r\n","      name = tf.compat.as_str_any(img_name).split(\"'\")[1].split(\"'\")[0]\r\n","      sizes = Image.open(test_path + \"/\" + name).size\r\n","\r\n","      print(sizes)\r\n","      pred_resize = finalPrediction.resize(sizes, resample=Image.NEAREST)\r\n","\r\n","      img_array = np.array(pred_resize)\r\n","      new_mask_arr = np.zeros(sizes[::-1] , dtype=img_array.dtype)\r\n","      new_mask_arr[np.where(np.all(img_array == [255, 255, 255], axis=-1))] = 1\r\n","      new_mask_arr[np.where(np.all(img_array == [216, 67, 82], axis=-1))] = 2\r\n","      \r\n","      mask_arr = new_mask_arr\r\n","      name = name[:-4]\r\n","\r\n","      print(f\"predicted {name}\")\r\n","      submission_dict[name] = {}\r\n","      submission_dict[name]['shape'] = mask_arr.shape\r\n","      submission_dict[name]['team'] = azienda\r\n","      submission_dict[name]['crop'] = tipo\r\n","      submission_dict[name]['segmentation'] = {}\r\n","\r\n","      # RLE encoding\r\n","      # crop\r\n","      rle_encoded_crop = rle_encode(mask_arr == 1)\r\n","      # weed\r\n","      rle_encoded_weed = rle_encode(mask_arr == 2)\r\n","\r\n","      submission_dict[name]['segmentation']['crop'] = rle_encoded_crop\r\n","      submission_dict[name]['segmentation']['weed'] = rle_encoded_weed\r\n","\r\n","with open(f'./submission.json', 'w') as f:\r\n","    json.dump(submission_dict, f)\r\n","    \r\n","\r\n"],"execution_count":null,"outputs":[]}]}